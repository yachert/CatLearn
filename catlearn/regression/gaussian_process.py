"""Functions to make predictions with Gaussian Processes machine learning."""
'''
GaussianProcess æ˜¯ CatLearn ä¸­çš„ GP å®ç°ï¼Œç”¨æ¥åš èƒ½é‡/åŠ›ï¼ˆæˆ–ä»»æ„ç›®æ ‡ï¼‰å›å½’ï¼Œæ”¯æŒï¼š
ç”¨ ç‰¹å¾å‘é‡ï¼ˆfingerprintsï¼‰ ä½œä¸ºè¾“å…¥ï¼›
åŒæ—¶æŠŠ èƒ½é‡ï¼ˆtargetï¼‰å’Œæ¢¯åº¦ï¼ˆforcesï¼‰ ä½œä¸ºè®­ç»ƒä¿¡æ¯ï¼ˆè‹¥æä¾› gradientsï¼‰ï¼›
è‡ªå®šä¹‰æ ¸ï¼ˆkernel_listï¼‰ï¼ˆå¤šä¸ªå­æ ¸ç»„åˆï¼‰å¹¶æ”¯æŒè¶…å‚æ•°ä¼˜åŒ–ï¼ˆåŸºäºå¯¹æ•°è¾¹é™…ä¼¼ç„¶æˆ–å…¶ä»–æŸå¤±ï¼‰ï¼›
æä¾› predict(...)ï¼ˆè¿”å›å‡å€¼ï¼‰å’Œ predict(..., uncertainty=True)ï¼ˆè¿”å›ä¸ç¡®å®šåº¦ï¼‰ï¼›
æ”¯æŒ scale_dataï¼ˆå¯¹ç‰¹å¾ä¸ target åšæ ‡å‡†åŒ–ï¼‰ï¼Œå¹¶èƒ½ä»¥ update_data é«˜æ•ˆæ›´æ–°è®­ç»ƒé›†ï¼›
ç›´æ¥è®¡ç®—å¹¶å­˜å‚¨ Gramï¼ˆåæ–¹å·®ï¼‰çŸ©é˜µçš„é€† cinvï¼Œç”¨äºé«˜æ•ˆé¢„æµ‹ã€‚
'''
from __future__ import absolute_import
from __future__ import division

import numpy as np
from scipy.optimize import minimize, basinhopping
from collections import defaultdict
import functools
import warnings
from .gpfunctions.log_marginal_likelihood import log_marginal_likelihood  # æœ€å¤§å¯¹æ•°è¾¹é™…ä¼¼ç„¶(LML)
from .gpfunctions.covariance import get_covariance
from .gpfunctions.kernel_setup import prepare_kernels, kdicts2list, list2kdict
from .gpfunctions.uncertainty import get_uncertainty 
from .gpfunctions.default_scale import ScaleData # å¦‚æœ scale_data=Trueï¼Œä¼šå¯¹è®­ç»ƒè¾“å…¥/è¾“å‡ºåšæ ‡å‡†åŒ–ï¼ˆå¯¹æ•°æˆ–çº¿æ€§æ ‡å‡†åŒ–ç”±å®ç°å†³å®šï¼‰
from .cost_function import get_error, _cost_function


class GaussianProcess(object):
    """Gaussian processes functions for the machine learning."""

    def __init__(self, train_fp, train_target, kernel_list, gradients=None,
                 regularization=None, regularization_bounds=None,
                 optimize_hyperparameters=False, scale_optimizer=False,
                 scale_data=False):
        # å‚æ•°ä¸åˆå§‹åŒ–æµç¨‹ï¼ˆé‡ç‚¹ï¼šå½¢çŠ¶/å«ä¹‰ï¼‰
        # train_fpï¼šè®­ç»ƒç‰¹å¾ï¼ŒäºŒç»´ç»“æ„ã€‚å¸¸è§å½¢çŠ¶ (M, D)ï¼šM = è®­ç»ƒç‚¹æ•°ï¼ŒD = æ¯ç‚¹ç‰¹å¾ç»´åº¦ã€‚å‡½æ•°æœ€å¼€å§‹ç”¨ assert np.shape(train_fp)[0] == len(train_target) æ£€æŸ¥æ ·æœ¬æ•°ä¸€è‡´ã€‚
        # train_targetï¼šè®­ç»ƒç›®æ ‡ï¼ˆe.g. èƒ½é‡ï¼‰ï¼Œé€šå¸¸ (M,) æˆ– (M,1)ã€‚
        # gradientsï¼šå¯é€‰ã€‚è‹¥æä¾›ï¼Œè¯´æ˜ä½ åŒæ—¶æœ‰æ¯ä¸ªè®­ç»ƒç‚¹çš„å¯¼æ•°ä¿¡æ¯ï¼ˆä¾‹å¦‚æ¯ç‚¹çš„åŠ›ï¼‰ã€‚å½¢çŠ¶é€šå¸¸ (M, 3N)ï¼ˆæ‰å¹³åŒ–ï¼‰æˆ–èƒ½è¢«è½¬æ¢ä¸ºä¸€ç»´å‘é‡è¿½åŠ åˆ°ç›®æ ‡ä¸Šï¼ˆè§ä¸‹æ–‡ how they appendï¼‰ã€‚
        # kernel_listï¼šä¸€ç»„ kernel é…ç½®ï¼ˆlist of dictï¼‰ï¼Œç”± prepare_kernels è§£ææˆå†…éƒ¨ç”¨çš„ self.kernel_list å’Œ self.boundsï¼ˆä¼˜åŒ–è¾¹ç•Œï¼‰ã€‚
                    #  æ¯ä¸ª kernel dict åŒ…å« 'type', 'width'/'lengthscale', 'scaling' ç­‰è¶…å‚æ•°ã€‚
        # regularizationï¼šåæ–¹å·®çŸ©é˜µçš„å¯¹è§’æ­£åˆ™åŒ–é¡¹ï¼ˆjitter/noiseï¼‰
        # regularization_boundsï¼šè¶…å‚ä¼˜åŒ–æ—¶ regularization çš„ä¸Šä¸‹ç•Œã€‚è‹¥ gradients å­˜åœ¨ï¼Œé»˜è®¤ bounds æ›´ä¿å®ˆï¼ˆ(1e-3, 1e3)ï¼‰
        # optimize_hyperparametersï¼šè‹¥ Trueï¼Œåœ¨æ„é€ æ—¶ä¼šè°ƒç”¨ optimize_hyperparameters() åšè¶…å‚ä¼˜åŒ–ã€‚
        """Gaussian processes setup.

        Parameters
        ----------
        train_fp : list
            A list of training fingerprint vectors.
        train_target : list
            A list of training targets used to generate the predictions.
        kernel_list : list
            This list can contain many dictionaries, each one containing
            parameters for separate kernels.
            Each kernel dict contains information on a kernel such as:
            -   The 'type' key containing the name of kernel function.
            -   The hyperparameters, e.g. 'scaling', 'lengthscale', etc.
        gradients : list
            A list of gradients for all training data.
        regularization : float
            The regularization strength (smoothing function) applied to the
            covariance matrix.
        regularization_bounds : tuple
            Optional to change the bounds for the regularization.
        optimize_hyperparameters : boolean
            Optional flag to optimize the hyperparameters.
        scale_optimizer : boolean
            Flag to define if the hyperparameters are log scale for
            optimization.
        scale_data : boolean
            Scale the training and test features as well as target values.
            Default is False.
        """
        # Perform some sanity checks.
        msg = 'The number of data does not match the number of targets.'
        assert np.shape(train_fp)[0] == len(train_target), msg

        _, self.N_D = np.shape(train_fp)       # self.N_D = Dï¼ˆç‰¹å¾ç»´åº¦ï¼‰
        self.regularization = regularization   # åæ–¹å·®çŸ©é˜µçš„å¯¹è§’æ­£åˆ™åŒ–é¡¹ï¼ˆjitter/noiseï¼‰
        self.gradients = gradients             # è‹¥æä¾›ï¼Œè¯´æ˜ä½ åŒæ—¶æœ‰æ¯ä¸ªè®­ç»ƒç‚¹çš„å¯¼æ•°ä¿¡æ¯ï¼ˆä¾‹å¦‚æ¯ç‚¹çš„åŠ›ï¼‰
        self.scale_optimizer = scale_optimizer 
        self.scale_data = scale_data

        # Set flag for evaluating gradients.
        self.eval_gradients = False
        if self.gradients is not None:
            self.eval_gradients = True

        # Set bounds on regularization during hyperparameter optimization.
        if regularization_bounds is None:
            regularization_bounds = (1e-6, None)
            if self.eval_gradients:
                regularization_bounds = (1e-3, 1e3)

        self.kernel_list, self.bounds = prepare_kernels(
            kernel_list, regularization_bounds=regularization_bounds,
            eval_gradients=self.eval_gradients, N_D=self.N_D
        )
        # self.kernel_listã€self.bounds = prepare_kernels(...) è¾“å‡º

        self.update_data(train_fp, train_target, gradients=self.gradients,
                         scale_optimizer=scale_optimizer)
        # self.update_data(train_fp, train_target, gradients=...) è¢«è°ƒç”¨ï¼Œæ„é€  Gram çŸ©é˜µå¹¶æ±‚é€†ï¼ˆself.cinvï¼‰ï¼Œå¹¶è®¡ç®—åˆå§‹ LMLï¼ˆè‹¥ target å­˜åœ¨ï¼‰

        if optimize_hyperparameters:
            self.optimize_hyperparameters()
        

    # ====================================
    # ï¼ˆæœ€å…³é”®çš„ APIï¼‰ç”¨å·²ç»è®­ç»ƒå¥½çš„ GPï¼ˆå½“å‰å¯¹è±¡ä¿å­˜çš„ self.cinvã€self.train_fpã€self.train_targetã€self.kernel_list ç­‰ï¼‰å¯¹ æµ‹è¯•é›†ç‰¹å¾ test_fp ç»™å‡º é¢„æµ‹å‡å€¼ï¼ˆposterior meanï¼‰ï¼Œ
    #  å¹¶å¯é€‰åœ°ç»™å‡º ä¸ç¡®å®šåº¦ï¼ˆposterior stdï¼‰ã€è®­ç»ƒ/éªŒè¯è¯¯å·®ã€ä»¥åŠåŸºäºå›ºå®šåŸºå‡½æ•°çš„ä¿®æ­£é¢„æµ‹ã€‚
    #  self.eval_gradientsï¼šæ˜¯å¦åœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº†æ¢¯åº¦ï¼ˆè‹¥ Trueï¼ŒçŸ©é˜µå°ºå¯¸æ›´å¤æ‚ï¼‰ã€‚
    def predict(self, test_fp, test_target=None, uncertainty=False, basis=None,
                get_validation_error=False, get_training_error=False,
                epsilon=None):
        """Function to perform the prediction on some training and test data.

        Parameters
        ----------
        test_fp : list
            A list of testing fingerprint vectors.
        test_target : list è®­ç»ƒç›®æ ‡å€¼ï¼ˆå¦‚èƒ½é‡ï¼‰
            A list of the the test targets used to generate the prediction
            errors.
        uncertainty : boolean
            Return data on the predicted uncertainty if True. Default is False.
        basis : function
            Basis functions to assess the reliability of the uncertainty
            predictions. Must be a callable function that takes a list of
            descriptors and returns another list.
        get_validation_error : boolean
            Return the error associated with the prediction on the test set of
            data if True. Default is False.
        get_training_error : boolean
            Return the error associated with the prediction on the training set
            of data if True. Default is False.
        epsilon : float
            Threshold for insensitive error calculation.

        Returns
        ----------
        data : dictionary
            Gaussian process predictions and meta data:

            prediction : vector
                Predicted mean.
            uncertainty : vector
                Predicted standard deviation of the Gaussian posterior.
            training_error : dictionary
                Error metrics on training targets.
            validation_error : dictionary
                Error metrics on test targets.
        """
        # Perform some sanity checks.
        if get_validation_error:
            msg = 'No test targets provided, can not return validation error.'
            assert test_target is not None, msg

        # Enforce np.array type for test data.
        test_fp = np.asarray(test_fp)  # test_fp æœ€ç»ˆæ˜¯ numpy.ndarrayï¼Œshape (n_test, D)ã€‚å¦‚æœ scale_dataï¼Œä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–ï¼ˆåŒè®­ç»ƒï¼‰å˜æ¢ã€‚å‰é¢çš„æ˜¯train_fp
        if self.scale_data:
            test_fp = self.scaling.test(test_fp)
        if test_target is not None:
            test_target = np.asarray(test_target)

        # Store input data.
        data = defaultdict(list)

        # ===========è¯¶å“Ÿæˆ‘å¤©å‘ï¼Œå¤ªå…³é”®äº†
        # ktb è¡¨ç¤º Kâˆ—Xï¼šæµ‹è¯•ç‚¹ï¼ˆrowsï¼‰ä¸è®­ç»ƒç‚¹ï¼ˆcolsï¼‰ä¹‹é—´çš„åæ–¹å·®ã€‚
        # å½¢çŠ¶ï¼šé€šå¸¸ (n_test, n_train)ã€‚
        # å¦‚æœ eval_gradients=Trueï¼ˆå³è®­ç»ƒä¸­åŒ…å«æ¢¯åº¦/forcesï¼‰ï¼Œget_covariance ä¼šè¿”å›æ‰©å±•çš„åæ–¹å·®ï¼Œå¯èƒ½åŒ…å« block ç»“æ„ï¼Œå¯¹åº”èƒ½é‡-èƒ½é‡ã€èƒ½é‡-åŠ›ã€åŠ›-èƒ½é‡å’ŒåŠ›-åŠ› çš„äº¤å‰åæ–¹å·®ã€‚
        # å½¢çŠ¶å°†å˜ä¸º (n_test_blocks, n_train_blocks)ï¼Œå…·ä½“å–å†³äºå…¶å†…éƒ¨å¦‚ä½•å±•å¹³æ¢¯åº¦ï¼ˆè¿™ç‚¹å¯ä»¥ç”¨ ktb.shape æ‰“å°éªŒè¯ï¼‰ã€‚
        # Calculate the covariance between the test and training datasets.
        ktb = get_covariance(kernel_list=self.kernel_list, matrix1=test_fp,
                             matrix2=self.train_fp, regularization=None,
                             log_scale=self.scale_optimizer,
                             eval_gradients=self.eval_gradients) # self.eval_gradientsï¼šæ˜¯å¦åœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº†æ¢¯åº¦ï¼ˆè‹¥ Trueï¼ŒçŸ©é˜µå°ºå¯¸æ›´å¤æ‚ï¼‰ã€‚
        # =============

        # Build the list of predictions. é¢„æµ‹å‡å€¼
        # ğ›¼=ğ¶invâ‹…ğ‘¦ï¼ˆè¿™é‡Œ target ä¸ºè®­ç»ƒç›®æ ‡å‘é‡ yï¼Œshape (n_train,1)ï¼‰
        #ğ‘“^âˆ—=ğ¾âˆ—ğ‘‹â‹…ğ›¼ è¿”å› predï¼ˆé¢„æµ‹å‡å€¼ï¼‰ï¼Œå…¶ æ•°å­¦å…¬å¼æ˜¯æ ‡å‡† GP çš„åéªŒå‡å€¼å…¬å¼ï¼š
        # ğœ‡âˆ—=ğ¾âˆ—ğ‘‹ ğ¾ğ‘‹ğ‘‹âˆ’1ğ‘¦
        # å½¢çŠ¶ï¼špred çš„ shape é€šå¸¸æ˜¯ (n_test, 1) æˆ– (n_test,)ï¼ˆå–å†³å®ç°ï¼‰ï¼›åœ¨ä»£ç ä¸­ pred æœ€åå¦‚æœ self.scale_data ä¼š rescale_targets(pred)ã€‚
        data['prediction'] = self._make_prediction(ktb=ktb, cinv=self.cinv,
                                                   target=self.train_target)
        

        # Calculate error associated with predictions on the test data.
        # Calculate error associated with predictions on the training data. è®¡ç®—è®­ç»ƒ / éªŒè¯è¯¯å·®ï¼ˆå¯é€‰ï¼‰
        # å¦‚æœ get_validation_errorï¼šä½¿ç”¨ get_error(prediction=data['prediction'], target=test_target, epsilon=epsilon) è®¡ç®—è¯¯å·®æŒ‡æ ‡ï¼ˆä¾‹å¦‚ RMSEã€MAEï¼‰ï¼›è¿”å› data['validation_error']ï¼ˆå­—å…¸ï¼Œå«å…·ä½“æŒ‡æ ‡ï¼‰ã€‚
        # å¦‚æœ get_training_errorï¼šå…ˆæ„é€  kt_train = get_covariance(..., matrix1=self.train_fp) = K_{XX}ï¼Œç„¶å train_prediction = _make_prediction(ktb=kt_train, cinv=self.cinv, target=self.train_target) (å³åœ¨è®­ç»ƒç‚¹å¤„çš„é¢„æµ‹)ï¼Œå†ç”¨ get_error æ¯”è¾ƒè®­ç»ƒç›®æ ‡ä¸ train_predictionã€‚
        # æ³¨ï¼šåœ¨æ•°å€¼ä¸Šï¼Œtrain_prediction ç†è®ºä¸Šç­‰äºè®­ç»ƒ targetsï¼ˆå¦‚æœæ— å™ªå£°å¹¶ä¸”æ•°å€¼ç²¾ç¡®ï¼‰ï¼Œä½†ç”±äº regularization/æ•°å€¼/scale å¯èƒ½æœ‰å·®å¼‚ï¼Œå› æ­¤è¿”å›è®­ç»ƒè¯¯å·®æ¥è¯„ä¼°æ‹Ÿåˆè´¨é‡ã€‚

        if get_validation_error:
            data['validation_error'] = get_error(prediction=data['prediction'],
                                                 target=test_target,
                                                 epsilon=epsilon)

            
        if get_training_error:
            # Calculate the covariance between the training dataset.
            kt_train = get_covariance(
                kernel_list=self.kernel_list, matrix1=self.train_fp,
                regularization=None, log_scale=self.scale_optimizer,
                eval_gradients=self.eval_gradients)

            # Calculate predictions for the training data.
            data['train_prediction'] = self._make_prediction(
                ktb=kt_train, cinv=self.cinv, target=self.train_target
            )

            # Calculated the error for the prediction on the training data.
            if self.scale_data:
                train_target = self.scaling.rescale_targets(self.train_target)
            else:
                train_target = self.train_target
            data['training_error'] = get_error(
                prediction=data['train_prediction'], target=train_target,
                epsilon=epsilon
            )

        # Calculate uncertainty associated with prediction on test data.
        if uncertainty:
            data['uncertainty'] = get_uncertainty(
                kernel_list=self.kernel_list, test_fp=test_fp,
                ktb=ktb, cinv=self.cinv,
                log_scale=self.scale_optimizer
            )

            data['uncertainty_with_reg'] = data['uncertainty'] + \
                self.regularization # uncertainty_with_reg åœ¨ç»“æœä¸Šé¢å¤–åŠ ä¸Š self.regularizationï¼ˆæŠŠæ­£åˆ™/å™ªå£°é¡¹åŠ å›åˆ°ä¸ç¡®å®šåº¦ä¸Šï¼Œè¡¨å¾è§‚æµ‹å™ªå£°æˆ–æ¨¡å‹ä¸ç¡®å®šæ€§ä¸‹é™ï¼‰ã€‚

            # Rescale uncertainty if needed.
            if self.scale_data:
                data['uncertainty'] *= self.scaling.target_data['std']
                data['uncertainty_with_reg'] *= self.scaling.target_data['std']
                
        # ç®€å•ç†è§£ï¼šbasis è®©ä½ åœ¨ GP å‡å€¼ä¸ŠåŠ ä¸Š çº¿æ€§/éçº¿æ€§å¯è§£é‡Šé¡¹ï¼Œè¿™å¯¹äºä¸ç¡®å®šåº¦è¯„ä¼°å’Œå½’çº³æ€§èƒ½æœ‰å¸®åŠ©ï¼ˆä¾‹å¦‚å»æ‰è¶‹åŠ¿å GP æ›´ä¸“æ³¨å»ºæ¨¡æ®‹å·®ï¼Œä»è€Œä¸ç¡®å®šåº¦ä¼°è®¡æ›´å¯é ï¼‰ã€‚
        if basis is not None: # åœ¨ GP çš„åŸºç¡€ä¸Šå†æ‹Ÿåˆä¸€ä¸ªåŸºå‡½æ•°ï¼ˆæ¯”å¦‚çº¿æ€§é¡¹ã€å·²çŸ¥çš„ç‰©ç†è¶‹åŠ¿ç­‰ï¼‰ï¼ŒæŠŠ GP ç”¨æ¥å»ºæ¨¡æ®‹å·®ï¼Œè€Œä¸æ˜¯ç›´æ¥å»ºæ¨¡å…¨éƒ¨ä¿¡å·ã€‚
            data['basis'] = self._fixed_basis(
                train=self.train_fp, test=test_fp, ktb=ktb, cinv=self.cinv,
                target=self.train_target, test_target=test_target, basis=basis,
                epsilon=epsilon
            )

        return data

    # ==============================================
    def predict_uncertainty(self, test_fp):
        """Return uncertainty only.

        Parameters
        ----------
        test_fp : list
            A list of testing fingerprint vectors.
        """
        # Calculate the covariance between the test and training datasets.
        ktb = get_covariance(kernel_list=self.kernel_list, matrix1=test_fp,
                             matrix2=self.train_fp, regularization=None,
                             log_scale=self.scale_optimizer,
                             eval_gradients=self.eval_gradients)
        # Store input data.
        data = defaultdict(list)

        data['uncertainty'] = get_uncertainty(
            kernel_list=self.kernel_list, test_fp=test_fp,
            ktb=ktb, cinv=self.cinv,
            log_scale=self.scale_optimizer)

        data['uncertainty_with_reg'] = data['uncertainty'] + \
            self.regularization

        # Rescale uncertainty if needed.
        if self.scale_data:
            data['uncertainty'] *= self.scaling.target_data['std']
            data['uncertainty_with_reg'] *= self.scaling.target_data['std']
        return data

    def update_data(self, train_fp, train_target=None, gradients=None,
                    scale_optimizer=False):
        """Update the training matrix, targets and covariance matrix.

        This function assumes that the descriptors in the feature set remain
        the same. That it is just the number of data ponts that is changing.
        For this reason the hyperparameters are not updated, so this update
        process should be fast.

        Parameters
        ----------
        train_fp : list
            A list of training fingerprint vectors.
        train_target : list
            A list of training targets used to generate the predictions.
        scale_optimizer : boolean
            Flag to define if the hyperparameters are log scale for
            optimization.

            
        å½¢çŠ¶æ£€æŸ¥ï¼šd, f = np.shape(train_fp)ï¼Œå¹¶æ–­è¨€ f == self.N_Dï¼ˆç‰¹å¾ç»´åº¦ä¸€è‡´ï¼‰ã€‚
        å­˜å‚¨è®­ç»ƒç‰¹å¾/ç›®æ ‡ï¼šself.train_fp = np.asarray(train_fp)ï¼›è‹¥ train_target éç©ºåˆ™ self.train_target = np.asarray(train_target)ã€‚
        scale_data åˆ†æ”¯ï¼ˆè‹¥ self.scale_data=Trueï¼‰ï¼š
        åˆ›å»º self.scaling = ScaleData(train_fp, train_target)ï¼Œå¹¶å¯¹ train_fp, train_target = self.scaling.train() æ ‡å‡†åŒ–ã€‚

        è‹¥æä¾› gradientsï¼ŒæŒ‰ç…§ç¼©æ”¾æ¯”ä¾‹å¯¹æ¢¯åº¦åšç­‰æ¯”ä¾‹ç¼©æ”¾ï¼šgradients = gradients / (std_target / std_feature)ï¼Œå¹¶ ravel æˆä¸€ç»´è¿½åŠ åˆ°ç›®æ ‡ï¼ˆå› ä¸ºè”åˆè®­ç»ƒèƒ½é‡+æ¢¯åº¦æ—¶å¸¸æŠŠæ¢¯åº¦ä½œä¸ºé¢å¤–ç›®æ ‡é¡¹æ‹¼æ¥ï¼‰ã€‚

        è‹¥æ—¢æœ‰ gradients åˆæœ‰ train_targetï¼šæŠŠ gradients flatten åç”¨ np.append æ‹¼æ¥åˆ° self.train_targetï¼Œå¹¶ reshape æˆåˆ—å‘é‡ã€‚

        è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šå®ç°æŠŠèƒ½é‡å’Œæ¢¯åº¦ä¸²æ¥æˆä¸€ä¸ªé•¿çš„ç›®æ ‡å‘é‡ï¼Œå½¢å¼ä¸Šæ˜¯æŠŠèƒ½é‡æ¡ç›®åœ¨å‰ã€æ‰€æœ‰æ¢¯åº¦æ¡ç›®åœ¨åï¼ˆå…·ä½“æ’åˆ—é¡ºåºå–å†³å®ç°ï¼‰ã€‚è¿™å…è®¸åœ¨ Gram çŸ©é˜µé‡ŒåŒæ—¶è¡¨ç¤ºèƒ½é‡-èƒ½é‡ã€èƒ½é‡-åŠ›ã€åŠ›-åŠ›çš„åæ–¹å·®å—ã€‚

        æ„é€  Gram çŸ©é˜µï¼šcvm = get_covariance(kernel_list=..., matrix1=self.train_fp, regularization=self.regularization, log_scale=scale_optimizer, eval_gradients=self.eval_gradients)ã€‚

        è¿™ä¸€æ­¥ä¼šæ ¹æ® kernel_list æ„é€ è®­ç»ƒç‚¹ä¹‹é—´ï¼ˆä»¥åŠè‹¥ eval_gradients=True æ—¶ï¼Œèƒ½é‡ä¸åŠ›ä¹‹é—´ï¼‰çš„å®Œæ•´åæ–¹å·®çŸ©é˜µï¼ˆé€šå¸¸å¤§å° = M*(1+3N?)ï¼Œè§†å®ç°å¦‚ä½•å±•å¹³æ¢¯åº¦ï¼‰ã€‚

        æ±‚é€†ï¼šself.cinv = np.linalg.inv(cvm)ã€‚

        æ³¨æ„ï¼šç›´æ¥æ±‚é€†æ˜¯æ•°å€¼/æ€§èƒ½ä¸Šä¸ä¼˜çš„ï¼ˆåº”è¯¥ç”¨ Cholesky + solveï¼‰ï¼Œä½†è¿™é‡Œå®ç°ç›´æ¥ç”¨ invã€‚è‹¥çŸ©é˜µæ¥è¿‘å¥‡å¼‚ï¼Œä¼šå‡ºé”™ã€‚self.regularization å°±æ˜¯ç”¨æ¥ä¿è¯æ­£å®šæ€§çš„ã€‚

        è‹¥æœ‰ train_targetï¼Œåˆ™è°ƒç”¨ _update_lml() è®¡ç®— log marginal likelihood å¹¶ä¿å­˜ï¼Œå¦åˆ™è­¦å‘Šâ€œGP mean not updatedâ€ã€‚
        """
        # Get the shape of the training dataset.
        d, f = np.shape(train_fp) # train_fpï¼šè®­ç»ƒç‰¹å¾ï¼ŒäºŒç»´ç»“æ„ã€‚å¸¸è§å½¢çŠ¶ (M, D)ï¼šM = è®­ç»ƒç‚¹æ•°ï¼ŒD = æ¯ç‚¹ç‰¹å¾ç»´åº¦ã€‚å‡½æ•°æœ€å¼€å§‹ç”¨ assert np.shape(train_fp)[0] == len(train_target) æ£€æŸ¥æ ·æœ¬æ•°ä¸€è‡´ã€‚

        # Perform some sanity checks.
        if self.N_D != f:
            msg = str(f) + '!=' + str(self.N_D)
            msg += '\n The number of features has changed. Train a new '
            msg += 'model instead of trying to update.'
            raise AssertionError(msg)

        # Store the training data in the GP, enforce np.array type.
        self.train_fp = np.asarray(train_fp)

        if train_target is not None:
            self.train_target = np.asarray(train_target)

        if self.scale_data:
            self.scaling = ScaleData(train_fp, train_target) # å¯¹ train_fp, train_target = self.scaling.train() æ ‡å‡†åŒ–
            self.train_fp, self.train_target = self.scaling.train()
            if gradients is not None:
                gradients = gradients / (self.scaling.target_data['std'] /
                                         self.scaling.feature_data['std'])
                gradients = np.ravel(gradients) # è‹¥æä¾› gradientsï¼ŒæŒ‰ç…§ç¼©æ”¾æ¯”ä¾‹å¯¹æ¢¯åº¦åšç­‰æ¯”ä¾‹ç¼©æ”¾ï¼šgradients = gradients / (std_target / std_feature)ï¼Œå¹¶ ravel æˆä¸€ç»´è¿½åŠ åˆ°ç›®æ ‡ï¼ˆå› ä¸ºè”åˆè®­ç»ƒèƒ½é‡+æ¢¯åº¦æ—¶å¸¸æŠŠæ¢¯åº¦ä½œä¸ºé¢å¤–ç›®æ ‡é¡¹æ‹¼æ¥ï¼‰ã€‚

        if gradients is not None and train_target is not None: 
            # è‹¥æ—¢æœ‰ gradients åˆæœ‰ train_targetï¼šæŠŠ gradients flatten åç”¨ np.append æ‹¼æ¥åˆ° self.train_targetï¼Œå¹¶ reshape æˆåˆ—å‘é‡ã€‚
            # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šå®ç°æŠŠèƒ½é‡å’Œæ¢¯åº¦ä¸²æ¥æˆä¸€ä¸ªé•¿çš„ç›®æ ‡å‘é‡ï¼Œå½¢å¼ä¸Šæ˜¯æŠŠèƒ½é‡æ¡ç›®åœ¨å‰ã€æ‰€æœ‰æ¢¯åº¦æ¡ç›®åœ¨åï¼ˆå…·ä½“æ’åˆ—é¡ºåºå–å†³å®ç°ï¼‰ã€‚è¿™å…è®¸åœ¨ Gram çŸ©é˜µé‡ŒåŒæ—¶è¡¨ç¤ºèƒ½é‡-èƒ½é‡ã€èƒ½é‡-åŠ›ã€åŠ›-åŠ›çš„åæ–¹å·®å—ã€‚
            train_target_grad = np.append(self.train_target, gradients)
            self.train_target = np.reshape(train_target_grad,
                                           (np.shape(train_target_grad)[0], 1))

        # Get the Gram matrix on-the-fly if none is suppiled.
        # è¿™ä¸€æ­¥ä¼šæ ¹æ® kernel_list æ„é€ è®­ç»ƒç‚¹ä¹‹é—´ï¼ˆä»¥åŠè‹¥ eval_gradients=True æ—¶ï¼Œèƒ½é‡ä¸åŠ›ä¹‹é—´ï¼‰çš„å®Œæ•´åæ–¹å·®çŸ©é˜µï¼ˆé€šå¸¸å¤§å° = M*(1+3N?)ï¼Œè§†å®ç°å¦‚ä½•å±•å¹³æ¢¯åº¦ï¼‰ã€‚
        cvm = get_covariance(
            kernel_list=self.kernel_list, matrix1=self.train_fp,
            regularization=self.regularization, log_scale=scale_optimizer,
            eval_gradients=self.eval_gradients)

        # Invert the covariance matrix. æ±‚é€†ï¼š
        # æ³¨æ„ï¼šç›´æ¥æ±‚é€†æ˜¯æ•°å€¼/æ€§èƒ½ä¸Šä¸ä¼˜çš„ï¼ˆåº”è¯¥ç”¨ Cholesky + solveï¼‰ï¼Œä½†è¿™é‡Œå®ç°ç›´æ¥ç”¨ invã€‚è‹¥çŸ©é˜µæ¥è¿‘å¥‡å¼‚ï¼Œä¼šå‡ºé”™ã€‚self.regularization å°±æ˜¯ç”¨æ¥ä¿è¯æ­£å®šæ€§çš„ã€‚
        self.cinv = np.linalg.inv(cvm)
        if train_target is None: # è‹¥æœ‰ train_targetï¼Œåˆ™è°ƒç”¨ _update_lml() è®¡ç®— log marginal likelihood å¹¶ä¿å­˜ï¼Œå¦åˆ™è­¦å‘Šâ€œGP mean not updatedâ€ã€‚
            warnings.warn("GP mean not updated.")
            self.log_marginal_likelihood = np.nan
        else:
            self._update_lml()
            
    # ===============================ï¼ˆè¶…å‚æ•°ä¼˜åŒ–ï¼‰
    def optimize_hyperparameters(self, global_opt=False, algomin='L-BFGS-B',
                                 eval_jac=False, loss_function='lml'):
        """Optimize hyperparameters of the Gaussian Process.

        This function assumes that the descriptors in the feature set remain
        the same. Optimization is performed with respect to the log marginal
        likelihood. Optimized hyperparameters are saved in the kernel
        dictionary. Finally, the covariance matrix is updated.

        Parameters
        ----------
        global_opt : boolean
            Flag whether to do basin hopping optimization of hyperparameters.
            Default is False.
        algomin : str
            Define scipy minimizer method to call. Default is L-BFGS-B.
        """
        # Create a list of all hyperparameters.
        theta = kdicts2list(self.kernel_list, N_D=self.N_D) # æŠŠ kernel_list çš„æ‰€æœ‰å¯ä¼˜åŒ–è¶…å‚æ‰“å¹³æˆå‘é‡ theta
        theta = np.append(theta, self.regularization)

        if loss_function == 'lml':
            # Define fixed arguments for log_marginal_likelihood
            args = (np.array(self.train_fp), np.array(self.train_target),
                    self.kernel_list, self.scale_optimizer,
                    self.eval_gradients, None, eval_jac)
            lf = log_marginal_likelihood
        elif loss_function == 'rmse' or loss_function == 'absolute':
            # Define fixed arguments for rmse loss function
            args = (np.array(self.train_fp), np.array(self.train_target),
                    self.kernel_list, self.scale_optimizer, loss_function)
            lf = _cost_function
        else:
            raise NotImplementedError(str(loss_function))
        # Optimize
        if not global_opt:
            self.theta_opt = minimize(lf, theta,
                                      args=args,
                                      method=algomin,
                                      jac=eval_jac,
                                      bounds=self.bounds)
        else:
            minimizer_kwargs = {'method': algomin, 'args': args,
                                'bounds': self.bounds, 'jac': eval_jac}
            self.theta_opt = basinhopping(lf, theta,
                                          T=10., interval=30, niter=30,
                                          minimizer_kwargs=minimizer_kwargs)

        # Update kernel_list and regularization with optimized values.
        self.kernel_list = list2kdict(self.theta_opt['x'][:-1],
                                      self.kernel_list)
        self.regularization = self.theta_opt['x'][-1]
        self.log_marginal_likelihood = -self.theta_opt['fun']
        # Make a new covariance matrix with the optimized hyperparameters.
        cvm = get_covariance(kernel_list=self.kernel_list,
                             matrix1=self.train_fp,
                             regularization=self.regularization,
                             log_scale=self.scale_optimizer,
                             eval_gradients=self.eval_gradients)
        # Invert the covariance matrix.
        self.cinv = np.linalg.inv(cvm)

    def update_gp(self, train_fp=None, train_target=None, kernel_list=None,
                  scale_optimizer=False, gradients=None,
                  regularization_bounds=(1e-6, None),
                  optimize_hyperparameters=False):
        """Potentially optimize the full Gaussian Process again.

        This alows for the definition of a new kernel as a result of changing
        descriptors in the feature space. Other parts of the model can also be
        changed. The hyperparameters will always be reoptimized.

        Parameters
        ----------
        train_fp : list
            A list of training fingerprint vectors.
        train_target : list
            A list of training targets used to generate the predictions.
        kernel_list : dict
            This dict can contain many other dictionarys, each one containing
            parameters for separate kernels.
            Each kernel dict contains information on a kernel such as:
            -   The 'type' key containing the name of kernel function.
            -   The hyperparameters, e.g. 'scaling', 'lengthscale', etc.
        scale_optimizer : boolean
            Flag to define if the hyperparameters are log scale for
            optimization.
        regularization_bounds : tuple
            Optional to change the bounds for the regularization.
        """
        if train_fp is not None:
            _, self.N_D = np.shape(train_fp)
            self.train_fp = np.asarray(train_fp)

        # Assign flags for gradient evaluation.
        eval_gradients = False
        if gradients is not None:
            eval_gradients = True

        if kernel_list is not None:
            self.kernel_list, self.bounds = prepare_kernels(
                kernel_list, regularization_bounds=regularization_bounds,
                eval_gradients=eval_gradients, N_D=self.N_D
            )
        if train_target is not None:
            msg = 'To update the data, both train_fp and train_target must be '
            msg += 'defined.'
            assert train_fp is not None, msg
            self.update_data(train_fp, train_target, gradients,
                             scale_optimizer)

        if optimize_hyperparameters:
            self.optimize_hyperparameters()
        else:
            self._update_lml()

    def _make_prediction(self, ktb, cinv, target): # ï¼ˆçŸ©é˜µè¿ç®—çš„æ ¸å¿ƒï¼‰
        """Function to make the prediction.

        Parameters
        ----------
        ktb : array
            Covariance matrix between test and training data.
        cinv : array
            Inverted Gram matrix, covariance between training data.
        target : list
            The target values for the training data.

        Returns
        -------
        pred : list
            The predictions for the test data.
        """
        # Form list of the actual predictions.
        # è¿™æ˜¯æ ‡å‡† GP çš„é—­å¼è§£ï¼ˆåœ¨ç»™å®šæ ¸ä¸åæ–¹å·®é€†çš„æƒ…å†µä¸‹ï¼‰ï¼Œå¤æ‚æ€§é›†ä¸­åœ¨ cinv çš„è®¡ç®—/ç¨³å®šæ€§ã€‚

        # # Step 1: è®¡ç®—æƒé‡å‘é‡ Î± = [K + ÏƒÂ²I]^(-1) Â· y
        alpha = functools.reduce(np.dot, (cinv, target)) 
        # # Step 2: é¢„æµ‹å‡å€¼ = K(X*, X) Â· Î±
        pred = functools.reduce(np.dot, (ktb, alpha))

        if self.scale_data:
            pred = self.scaling.rescale_targets(pred)

        return pred

    def _fixed_basis(self, test, train, basis, ktb, cinv, target, test_target,
                     epsilon):
        """Function to apply fixed basis.

        Returns
        -------
            Predictions gX on the residual.
        """
        data = defaultdict(list)
        # Calculate the K(X*,X*) covariance matrix.
        ktest = get_covariance(
            kernel_list=self.kernel_list, matrix1=test, regularization=None,
            log_scale=self.scale_optimizer, eval_gradients=self.eval_gradients)

        # Form H and H* matrix, multiplying X by basis.
        train_matrix = np.asarray([basis(i) for i in train])
        test_matrix = np.asarray([basis(i) for i in test])

        # Calculate R.
        r = test_matrix - ktb.dot(cinv.dot(train_matrix))

        # Calculate beta.
        b1 = np.linalg.inv(train_matrix.T.dot(cinv.dot(train_matrix)))
        b2 = np.asarray(target).dot(cinv.dot(train_matrix))
        beta = b1.dot(b2)

        # Form the covariance function based on the residual.
        covf = ktest - ktb.dot(cinv.dot(ktb.T))
        gca = train_matrix.T.dot(cinv.dot(train_matrix))
        data['g_cov'] = covf + r.dot(np.linalg.inv(gca).dot(r.T))

        # Do prediction accounting for basis.
        data['gX'] = self._make_prediction(ktb=ktb, cinv=cinv, target=target) \
            + beta.dot(r.T)

        # Calculated the error for the residual prediction on the test data.
        if test_target is not None:
            data['validation_error'] = get_error(prediction=data['gX'],
                                                 target=test_target,
                                                 epsilon=epsilon)

        return data

    def _update_lml(self):
        # Create a list of all hyperparameters.
        theta = kdicts2list(self.kernel_list, N_D=self.N_D)
        theta = np.append(theta, self.regularization)
        # Update log marginal likelihood.
        self.log_marginal_likelihood = -log_marginal_likelihood(
                theta=theta,
                train_matrix=np.array(self.train_fp),
                targets=np.array(self.train_target),
                kernel_list=self.kernel_list,
                scale_optimizer=self.scale_optimizer,
                eval_gradients=self.eval_gradients,
                cinv=self.cinv,
                eval_jac=False)
